## PART 1: Raichu

- The idea in this game was to play a game of checkers with a twist. 

- There are various methods which are specificaly designated for determining the next piece moves. They are:
    1. `get_pichu_moves`: This method is to get all the possible moves that a pichu can make. There are four distinct moves that the pichu can make.
        - Row Down-Column Left
        - Row Down-Column Right
        - Row Down Down-Column Left Left
        - Row Down Down-Column Right Right   
    2. `get_pikachu_moves`: Similar to above, this method gives all the moves that a pikachu can make. A Pikachu can make moves in 3 Squares, 2 Squares and 1 Square along with additional directions(left or right).
    4. `get_raichu_moves`: This method is similar to the previously defined method. The only change being that a Raichu is capable of making moves in all the four directions (Forward, Backward, Left and Right) and it also can traverse indefinite squares until a new piece is encountered. We dynamically generate the moves for a raichu possible by iterating over all possible row, column and diagonal moves. 

- The `successors()` method gets all the possible succesors.

- In order to prioritize the states generated by the successor function, we define another method `calculate_cost` which calculates the total value of the pieces.

- The two main functions which make this program are the methods used to expand minimum tree and the maximum tree. These functions are basic complements of each other and perform similar tasks except that they are mirrored in the way they calculate and prioritize the values/cost.

- The created a dictionary which would store the results of all the successors. In order to reduce the running time, we use caching.

## PART 2: The Game of Quintris

- To start with this part of the homework, it requires us to modify the super simple algorithm given in the skeleton with something better to create a Computer player, able to play the game of Qunitris.

- The game of Quintris is basically a one-up version of the nostalgic Tetris game. The only change being that each piece in Quintris (as the name suggests) is built using 5 blocks.

- To start with this, we create a method `grab_matrix` which just acts as a helper method to grab a smaller matrix from a bigger matrix

- The next method is the `apply_mask` method. The idea behind this was to find a create 'masks' on the basis of the current arrangements on the board. For instance for a piece 'xxxxx' to be put on the board, we check for an availability of 'ooooo' where 'o' is an empty space. We create such masks and try to fit the pieces with the lowest costs. To make this even easier, we define two other methods which are `convert_piece_to_matrix` and `convert_matrix_to_piece` which makes it easier to create and apply masks in terms of matrices.

- We also need to check whether the mask fits on the target or not. To do this a method `does_piece_fit` has been created. The only catch here is that we need to make sure that the piece matches the target.

- To reduce the costs, another factor that we need to consider is to rotate the pieces clockwise in phases of 90, 180, and 270 degrees. In order to implement this, we borrowed a very neat piece of code from https://stackoverflow.com/questions/8421337/rotating-a-two-dimensional-array-in-python

- Calculating Loss and Gain: A loss occurs when a a blank space is below another 'x' tile ina particular piece. On the other hand, a gain happens when a blank space is above another 'x' tile in a piece so that the other tiles falling can possible it in there.

- In order to calculate the costs, we have used four different heuristic functions. 
    1. `piece_mask_cost`:  This calculates the gain and loss by each piece, while also checking if the piece fits. If the piece doesn't fit then we return infinity loss and gain
    2. `piece_row_elimination`: This counts the row elimination totals by baiscally counting the number of 'x' tiles in each row the piece sits in. This heuristic function ensures that pieces which can cut more rows are chosen.
        We were sorting this and returning the max at a point earlier, but we now sum it up outside when used
        `piece_row_elimination_gain_list.sort(key=lambda x: -x[1])`
    3. `get_shadow_value`: This gets the blank spaces underneath the pieces.
    4. `get_stack_height`: This heuristic gets the maximum height of the tiles reached so far. We also control the amount of height a piece adds to the board--but it usually turns out to be a bad strategy.  

- We have also defined methods to calculate costs and find other mask applications. This method goes through each row until the piece starts to fit in all columns, basically if it fits everywhere that is the time to stop computing.

## PART 3: Truth be told!

- The objective of this part of the homework is to create a Naive Bayes Classifier and train it on the training data (Customer Reviews) and then test the accuracy of the classifier by giving it the testing data as the input.

- To break down the training and testing data--we have labels, objects and classes.
 1) Labels: There are two types of labels: (1) Truthful and (2) Deceptive. It is the first word of each line on the data 
 2) Objects: They are the sentences preceded by the labels on each line of the data. They are multiple strings (words) joined together with spaces.
 3) Classes: They are the unique labels present in the training and testing data.

- We created multiple data dictionaries/lists for each label for storing data and making it easier to access as and when required. This eliminated the need to compute certain things again and again.

- There was a need to pre-process the data before passing it to the classifier. Tokenizing was done with the `re` module, which supports the use of regular expressions (Regex) in Python. The `re.split()` method is used to split the objects into words based on the non-alphanumeric characters present. In addition to this, we converted all the words in the object into a lowercase string using the `lower()` method. This is because Python interprets 'Hello' and 'hello' as different entities, but in the case of a text classification model, they should be interpreted the same so that equal probabilities can be assigned to them. Moreover, this decreases the redundancies in the dataset and improves the accuracy.

- Prior probabilities and likelihood probabilities for each word were calculated using Bayes' Theorem. 

- We imported a text file made of some common stop words that do not have any heavy implications on either of the two labels. These words are thus called Neutral words/Stop words. This list includes articles, pronouns, prepositions, conjunctions, and helping words, etc.

- To test the classifier, we passed the data object by object from the testing data, performed the same pre-processing techniques, and the classifier gave us the label by calculating the probability of each word in that label. After doing this for both the labels, the model picks the maximum probability computed from both the labels, and that is the label of the test sentence. 

- To compute these probabilities, we take the logarithmic sum of the values instead of the product as it avoids making the operands smaller. 

- We also tweaked the formula of Naive Bayes to include a 'smoothing factor', which handles the problem of encountering missing words in the test file. The formula is given by: (Number of occurrence of a particular word in a particular class + smoothing factor)/ ((total number of words in that class) + (number of classes * smoothing factor)). This is known as  Laplace Smoothing. We experimentally took the smoothing factor as '1'. The smoothing factor basically smoothens(normalises) the curve of the graph containing the probabilities of all the words.
